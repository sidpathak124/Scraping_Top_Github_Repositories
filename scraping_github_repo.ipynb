{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2565564",
   "metadata": {},
   "source": [
    "# Scraping the top repositories for various topics on github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd04a7",
   "metadata": {},
   "source": [
    "**TODO:**<br>\n",
    "- Introduction about web scraping\n",
    "- Introduction about github and the problem statement\n",
    "- The tools we will be using: Python, requests, Beautiful Soup, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5478010",
   "metadata": {},
   "source": [
    "**Project Strategy:**\n",
    "* We will scrape: https://github.com/topics (To get a list of topics)\n",
    "* For each topic we will get: Topic title, Topic URL, Topic description\n",
    "* For each topic we will get top 30 repositories\n",
    "* For each repository we will get: \n",
    "  repo name, username, stars, repo url\n",
    "* We will store all this data in a csv file, with a separate csv file for each topic\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4f9fd",
   "metadata": {},
   "source": [
    "### Scrape the list of topics from github's topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/upgrade requests\n",
    "\n",
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0eb33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/upgrade beautifulsoup\n",
    "\n",
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e7726",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030946b",
   "metadata": {},
   "source": [
    "### Get info on all the topics from the topics page(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the topic page (using below 3 helper functions)\n",
    "\n",
    "def scrape_topic_titles():\n",
    "    topics_dict = {\n",
    "        'title': [],\n",
    "        'description': [],\n",
    "        'url': []\n",
    "    }\n",
    "    \n",
    "    for page_num in range(1,7):\n",
    "        topics_url = f'https://github.com/topics?page={page_num}'\n",
    "        response = requests.get(topics_url)\n",
    "        while response.status_code != 200:   # if the webpage doesn't load, try again after 1 second\n",
    "            time.sleep(1)\n",
    "            response = requests.get(topics_url)\n",
    "\n",
    "        rc = response.text\n",
    "        doc = BeautifulSoup(rc, 'html.parser') \n",
    "        \n",
    "        topics_dict['title'] += get_topic_titles(doc)\n",
    "        topics_dict['description'] += get_topic_descriptions(doc)\n",
    "        topics_dict['url'] += get_topic_urls(doc)\n",
    "    \n",
    "    topics_df = pd.DataFrame(topics_dict)\n",
    "    return topics_df\n",
    "\n",
    "scrape_topic_titles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704c8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to help download the topic page\n",
    "\n",
    "# get titles\n",
    "def get_topic_titles(doc):\n",
    "    topic_title_tags = doc.find_all('p', {'class': \"f3 lh-condensed mb-0 mt-1 Link--primary\"}) \n",
    "    topic_titles = [topic_title_tags[i].text for i in range(len(topic_title_tags))]\n",
    "    return topic_titles\n",
    "\n",
    "# get urls\n",
    "def get_topic_urls(doc):\n",
    "    topic_title_tags = doc.find_all('p', {'class': \"f3 lh-condensed mb-0 mt-1 Link--primary\"}) \n",
    "    topic_urls = ['https://github.com' + topic_title_tags[i].parent['href'] for i in range(len(topic_title_tags))]\n",
    "    return topic_urls\n",
    "\n",
    "# get descriptions\n",
    "def get_topic_descriptions(doc):\n",
    "    topic_descr_tags = doc.find_all('p', {'class': \"f5 color-fg-muted mb-0 mt-1\"})\n",
    "    topic_descrs = [topic_descr_tags[i].text.strip() for i in range(len(topic_descr_tags))]\n",
    "    return topic_descrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (helper fn) converts \"num of stars\" from string to number \n",
    "\n",
    "def parse_stars_count(num_stars):\n",
    "    num_stars = num_stars.strip()\n",
    "    if num_stars[-1] == 'k':\n",
    "        int_num_stars = int(float(num_stars[:-1])*1000)\n",
    "        return int_num_stars\n",
    "    return int(num_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67061fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (helper fn) fetches the beautified html text from a given url\n",
    "\n",
    "def get_topic_doc(topic_url):\n",
    "    # request html for the given topic link\n",
    "    response = requests.get(topic_url)\n",
    "    # if it fails to fetch data\n",
    "    if response.status_code != 200:   \n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "    # if it successfully fetches data \n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Main fn) pass the beautified html text and it will return a dataframe with info\n",
    "\n",
    "def get_topic_info(topic_doc):\n",
    "    \"\"\"returns a dataframe containing info on all repos for the chosen topic\"\"\"\n",
    "    # get h3 tags containing usernames, repo names and urls\n",
    "    repo_tags = topic_doc.find_all('h3', {'class':'f3 color-fg-muted text-normal lh-condensed'})\n",
    "    total_tags = len(repo_tags)\n",
    "    # parse usernames\n",
    "    usernames = [repo_tags[i].find_all('a')[0].text.strip() for i in range(len(repo_tags))]\n",
    "    # parse repo names\n",
    "    repo_names = [repo_tags[i].find_all('a')[1].text.strip() for i in range(len(repo_tags))]\n",
    "    # parse urls for repos\n",
    "    base_url = 'https://github.com'\n",
    "    repo_urls = [base_url + repo_tags[i].find_all('a')[1]['href'] for i in range(len(repo_tags))]\n",
    "    \n",
    "    # get span tags containing num of stars\n",
    "    stars_tags = topic_doc.find_all('span', {'class': 'Counter js-social-count'})\n",
    "    # parse num of stars\n",
    "    num_stars = [stars_tags[i].text for i in range(len(stars_tags))]\n",
    "    stars = list(map(parse_stars_count, num_stars))\n",
    "    \n",
    "    # create dictionary for above four data points\n",
    "    topic_repos_dict = {'username':usernames, 'repo_name': repo_names, 'stars': stars, 'url': repo_urls}\n",
    "    # convert to dataframe\n",
    "    topic_info_df = pd.DataFrame(topic_repos_dict)\n",
    "    return topic_info_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e078d75a",
   "metadata": {},
   "source": [
    "### Get the top 30 repositories from a topic page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c98c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using topic url and title, scrape repos of that topic and store them as \"topic_title.csv\"\n",
    "\n",
    "def scrape_topic(topic_url, topic_title):\n",
    "    fname = './topics_csv/' + topic_title + '.csv'\n",
    "    if os.path.exists(fname):\n",
    "        print(f'The file: \"{fname}\", already exists. Skipping...')\n",
    "        return\n",
    "    topic_df = get_topic_info(get_topic_doc(topic_url))\n",
    "    topic_df.to_csv(fname, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1835d2",
   "metadata": {},
   "source": [
    "### Function for scraping, using all the above defined functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_topic_repos():\n",
    "    topics_df = scrape_topic_titles()\n",
    "    print('Scraping top repos of all topics from github')\n",
    "    if not os.path.exists(\"topics_csv\"):\n",
    "        os.mkdir(\"topics_csv\")\n",
    "    for ix, rows in topics_df.iterrows():\n",
    "        topic_url = rows['url']\n",
    "        topic_title = rows['title']\n",
    "        print(f'Scraping top repositories for topic: {topic_title}...')\n",
    "        scrape_topic(topic_url, topic_title)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebc4ff",
   "metadata": {},
   "source": [
    "### Execute scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_topic_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1a63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
